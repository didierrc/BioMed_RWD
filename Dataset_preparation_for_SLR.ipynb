{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny51RNr0S2V1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45a1f983"
      },
      "source": [
        "Let's add a list with our thesaurus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Thesaurus dictionary\n",
        "thesaurus_dic = {\n",
        "    \"blood\": [\"bld\", \"plasma\", \"plas\",\"serum\",\"ser\"],\n",
        "    \"plasma\": [\"plas\"],\n",
        "    \"white blood cells\": [\"wbc\",\"leukocytes\", \"lymphocytes\", \"monocytes\"]\n",
        "}\n",
        "\n",
        "print(thesaurus_dic[\"white blood cells\"])"
      ],
      "metadata": {
        "id": "IfycHjhEUBH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89a14680"
      },
      "source": [
        "Load data from the loinc_dataset document."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('RWD_loinc_dataset_wbc.csv')\n",
        "\n",
        "# Checking db's head\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "rZz7OQX3Vbld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "187f27f8"
      },
      "source": [
        "We introduce our query and a list with the different composites we will search with"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "querry = \"white blood cells count\"\n",
        "query_id = \"white_blood_cells_count\"\n",
        "composites = [\"white blood cells\", \"blood\", \"cells\",\"count\"]"
      ],
      "metadata": {
        "id": "IM8J1WA8XbO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa287d3e"
      },
      "source": [
        "## First Dataset\n",
        "We need to calculate our elementary clues X for all composite/document pairs\n",
        "\n",
        "For each composite, we will look for documents where it appears in the name, component, and system, and count how many times each composite appears in each of the three properties of each row. X1 is the number of times it appears in long_common_name, X3 the number of times it appears in component, and X4 the number of times it appears in system. Our Y is the is relevant field, X2 is the IDF (which I will program myself), querry_id is always \"white_blood_cells_count\", and composite is the composite used."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure elementary_clues_df is empty before filling\n",
        "elementary_clues_df = pd.DataFrame(columns=['loinc_num', 'query_id', 'composite_clue', 'TF', 'IDF', 'is_in_component', 'is_in_system', 'relevance'])\n",
        "\n",
        "# Calculate total number of documents\n",
        "N = len(df)\n",
        "\n",
        "# Calculate document frequency for original composites (for IDF of the composite itself)\n",
        "doc_freq_composites = {}\n",
        "for composite in composites:\n",
        "    lower_composite = composite.lower()\n",
        "    # Get the composite and its synonyms, including the composite itself\n",
        "    terms_to_check_doc_freq = [lower_composite]\n",
        "    if composite in thesaurus_dic:\n",
        "        terms_to_check_doc_freq.extend([synonym.lower() for synonym in thesaurus_dic[composite]])\n",
        "\n",
        "    # Count the number of documents where ANY of the terms in this group appear (using whole word match)\n",
        "    count = df.apply(lambda row: any(re.search(r'\\b' + re.escape(term) + r'\\b', str(row['long_common_name']).lower()) or\n",
        "                                     re.search(r'\\b' + re.escape(term) + r'\\b', str(row['component']).lower()) or\n",
        "                                     re.search(r'\\b' + re.escape(term) + r'\\b', str(row['system']).lower()) for term in terms_to_check_doc_freq), axis=1).sum()\n",
        "    doc_freq_composites[composite] = count\n",
        "\n",
        "# Pre-calculate terms to count for each original composite (composite + its synonyms)\n",
        "terms_to_count_map = {}\n",
        "for composite in composites:\n",
        "    lower_composite = composite.lower()\n",
        "    terms = [lower_composite]\n",
        "    if composite in thesaurus_dic:\n",
        "        terms.extend([synonym.lower() for synonym in thesaurus_dic[composite]])\n",
        "    terms_to_count_map[composite] = terms\n",
        "\n",
        "\n",
        "# Iterate over each row in the main DataFrame (df)\n",
        "for index, row in df.iterrows():\n",
        "    loinc_num = row['loinc_num']\n",
        "    long_common_name = str(row['long_common_name']).lower()\n",
        "    component = str(row['component']).lower()\n",
        "    system = str(row['system']).lower()\n",
        "    relevance = row['relevance'] # This will be our Y\n",
        "\n",
        "    # Iterate over each original composite in the composites_pre list\n",
        "    for composite in composites:\n",
        "        lower_composite = composite.lower()\n",
        "        terms_to_count = terms_to_count_map[composite]\n",
        "\n",
        "        # Check if ANY term related to this composite (composite or synonyms) is present in the document (using whole word match)\n",
        "        any_term_found = any(re.search(r'\\b' + re.escape(term) + r'\\b', long_common_name) or\n",
        "                             re.search(r'\\b' + re.escape(term) + r'\\b', component) or\n",
        "                             re.search(r'\\b' + re.escape(term) + r'\\b', system) for term in terms_to_count)\n",
        "\n",
        "\n",
        "        # If at least one related term was found in the document\n",
        "        if any_term_found:\n",
        "            # Calculate the total counts for X1, X3, X4 by summing occurrences of all related terms in the document (using whole word match)\n",
        "            total_x1 = sum(len(re.findall(r'\\b' + re.escape(term) + r'\\b', long_common_name)) for term in terms_to_count)\n",
        "\n",
        "            x3 = int(sum(len(re.findall(r'\\b' + re.escape(term) + r'\\b', component)) for term in terms_to_count) > 0)\n",
        "            x4 = int(sum(len(re.findall(r'\\b' + re.escape(term) + r'\\b', system)) for term in terms_to_count) > 0)\n",
        "\n",
        "            # Calculate X2 (IDF) for the ORIGINAL composite term\n",
        "            x2 = np.log10(N / (doc_freq_composites.get(composite, 0)))\n",
        "\n",
        "            # Create a new row dictionary for the original composite in this document\n",
        "            new_row = {\n",
        "                'loinc_num': loinc_num,\n",
        "                'query_id': query_id,\n",
        "                'composite_clue': composite,\n",
        "                'TF': total_x1,\n",
        "                'IDF': x2,\n",
        "                'is_in_component': x3,\n",
        "                'is_in_system': x4,\n",
        "                'relevance': relevance\n",
        "            }\n",
        "            # Add the new row to the elementary_clues_df DataFrame\n",
        "            elementary_clues_df.loc[len(elementary_clues_df)] = new_row\n",
        "\n",
        "display(elementary_clues_df)\n",
        "\n",
        "elementary_clues_df.to_csv('elementary_clues.csv', index=False, decimal=',')"
      ],
      "metadata": {
        "id": "P8N24VO1cwVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Dataset\n",
        "\n",
        "Our second dataset will have 5 columns\n",
        "\n",
        "1.   loinc_number\n",
        "2.   querry_id\n",
        "3.   Z → which is the initial relevance (calculed in the first logarithmic regresion)\n",
        "4.   N → which is the number of composites on a document\n",
        "5.   relevance (Y)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lkU9FyK54EIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the second dataset DataFrame\n",
        "second_dataset_df = pd.DataFrame(columns=['loinc_num', 'query_id', 'Z', 'N', 'relevance'])\n",
        "\n",
        "# Iterate over each row in the original DataFrame (df)\n",
        "for index, row in df.iterrows():\n",
        "    loinc_num = row['loinc_num']\n",
        "    long_common_name = str(row['long_common_name']).lower()\n",
        "    component = str(row['component']).lower()\n",
        "    system = str(row['system']).lower()\n",
        "    relevance = row['relevance']\n",
        "    query_id = \"white_blood_cells_count\"\n",
        "\n",
        "    # Calculate N: number of unique composites or their thesaurus found in long_common_name, component, or system\n",
        "    found_composites = set()\n",
        "    # Iterate over each original composite in the composites list\n",
        "    for composite in composites:\n",
        "        lower_composite = composite.lower()\n",
        "        # Get the composite and its synonyms, including the composite itself\n",
        "        terms_to_check = [lower_composite]\n",
        "        if composite in thesaurus_dic:\n",
        "            terms_to_check.extend([synonym.lower() for synonym in thesaurus_dic[composite]])\n",
        "\n",
        "        # Check if ANY of the terms related to this composite (composite or synonyms) is present in the document (using whole word match)\n",
        "        if any(re.search(r'\\b' + re.escape(term) + r'\\b', long_common_name) or\n",
        "               re.search(r'\\b' + re.escape(term) + r'\\b', component) or\n",
        "               re.search(r'\\b' + re.escape(term) + r'\\b', system) for term in terms_to_check):\n",
        "            found_composites.add(composite)\n",
        "\n",
        "\n",
        "    n_count = len(found_composites)\n",
        "\n",
        "    # Create a new row dictionary for the second dataset\n",
        "    new_row = {\n",
        "        'loinc_num': loinc_num,\n",
        "        'query_id': query_id,\n",
        "        'Z': 0,  # Z is always 0\n",
        "        'N': n_count,\n",
        "        'relevance': relevance\n",
        "    }\n",
        "\n",
        "    # Add the new row to the second_dataset_df DataFrame\n",
        "    second_dataset_df.loc[len(second_dataset_df)] = new_row\n",
        "\n",
        "# Display the second dataset DataFrame\n",
        "display(second_dataset_df)\n",
        "second_dataset_df.to_csv('second_ds.csv', index=False, decimal=',')"
      ],
      "metadata": {
        "id": "a5xVS4x35N5L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}