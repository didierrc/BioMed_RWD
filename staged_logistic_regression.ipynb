{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c99b4510",
      "metadata": {
        "id": "c99b4510"
      },
      "source": [
        "# BioMed: Real World Data - Staged Logistic Regression Model\n",
        "\n",
        "---\n",
        "\n",
        "**Group:**\n",
        "- González Méndez, Alvaro (alvaro.gmendez@alumnos.upm.es)\n",
        "- Reyes Castro, Didier Yamil (didier.reyes.castro@alumnos.upm.es)\n",
        "- Rodriguez Fernández, Cristina (cristina.rodriguezfernandez@alumnos.upm.es)\n",
        "\n",
        "**Course:** BioMedical Informatics - 2025/26\n",
        "\n",
        "**Institution:** Polytechnic University of Madrid (UPM)\n",
        "\n",
        "**Date:** October 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Goals\n",
        "\n",
        "The goal of the assignment is to implement a staged logistic regression model with real-world biomedical data. The model will be used to rank LOINC documents based on their relevance to specific clinical queries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ee411b",
      "metadata": {
        "id": "f1ee411b"
      },
      "source": [
        "## 0 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3afab43",
      "metadata": {
        "id": "f3afab43"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "import numpy as np\n",
        "import math\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5018fb21",
      "metadata": {
        "id": "5018fb21"
      },
      "source": [
        "## 1 Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791b1bda",
      "metadata": {
        "id": "791b1bda"
      },
      "outputs": [],
      "source": [
        "# Loading datasets\n",
        "\n",
        "DATASET_FIRST_STAGE = 'data/first_stage_data.csv'\n",
        "DATASET_SECOND_STAGE = 'data/second_stage_data.csv'\n",
        "\n",
        "MODEL_1_PATH = 'first_stage_logistic_regression_model.joblib'\n",
        "MODEL_2_PATH = 'second_stage_logistic_regression_model.joblib'\n",
        "\n",
        "try:\n",
        "    df_first_stage = pd.read_csv(DATASET_FIRST_STAGE, decimal=',')\n",
        "    df_second_stage = pd.read_csv(DATASET_SECOND_STAGE, decimal=',')\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading datasets: {e}\")\n",
        "    exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c73eced",
      "metadata": {
        "id": "7c73eced"
      },
      "source": [
        "### 1.1 Part A: Train First Logistic Regression Model (Intra-Clue)\n",
        "\n",
        "The elementary clues taken into account for the first stage are: TF, IDF, is_in_component and is_in_system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1bcc82b",
      "metadata": {
        "id": "a1bcc82b"
      },
      "outputs": [],
      "source": [
        "features_1 = ['TF', 'IDF', 'is_in_component', 'is_in_system']\n",
        "target_1 = 'relevance'\n",
        "\n",
        "X1 = df_first_stage[features_1]\n",
        "Y1 = df_first_stage[target_1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e8d3ad7",
      "metadata": {
        "id": "5e8d3ad7"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression default parameters: penalty='l2', C=1.0, solver='lbfgs'\n",
        "# solver can be changed to 'liblinear' as it is great for small datasets and binary\n",
        "# classification. Check: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "model_1 = LogisticRegression()\n",
        "model_1.fit(X1, Y1)\n",
        "\n",
        "# Save the trained model to a file\n",
        "# joblib.dump(model_1, MODEL_1_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b934fdbb",
      "metadata": {
        "id": "b934fdbb"
      },
      "source": [
        "### 1.2 Part B: Generate Second-Level Dataset\n",
        "\n",
        "1. Get Log-Odds from First Model (use the first dataset and predict with the model -> this will give you the log O(R/Ai) )\n",
        "2.  Sum up Log-Odds per Document (group by doc_id and sum the log-odds) -> Gives you the Z score per document.\n",
        "3. Complete the second stage dataset with the Z score (for those documents with 0 clues (N) fill Z with 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f772742e",
      "metadata": {
        "id": "f772742e"
      },
      "outputs": [],
      "source": [
        "# 1. Get log-Odds\n",
        "df_first_stage['log_odds'] = model_1.decision_function(X1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95afcbf",
      "metadata": {
        "id": "c95afcbf"
      },
      "outputs": [],
      "source": [
        "# 2. Calculate Z score per document\n",
        "Z = df_first_stage.groupby(['loinc_num', 'query_id'])['log_odds'].sum().reset_index()\n",
        "Z = Z.rename(columns={'log_odds': 'Z'})\n",
        "print(Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e21ca747",
      "metadata": {
        "id": "e21ca747"
      },
      "outputs": [],
      "source": [
        "# 3. Complete the second stage dataset with the Z score (for those documents with 0 clues (N) fill Z with 0)\n",
        "df_second_stage = df_second_stage.merge(Z, on=['loinc_num', 'query_id'], how='left')\n",
        "df_second_stage['Z'] = df_second_stage['Z'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_second_stage)"
      ],
      "metadata": {
        "id": "9GPS6EIvLx9x"
      },
      "id": "9GPS6EIvLx9x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "706e1b8d",
      "metadata": {
        "id": "706e1b8d"
      },
      "source": [
        "### 1.3 Part C: Train Second Logistic Regression Model (Inter-Clue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e87da57",
      "metadata": {
        "id": "8e87da57"
      },
      "outputs": [],
      "source": [
        "features_2 = ['Z', 'N']\n",
        "target_2 = 'relevance'\n",
        "\n",
        "X2 = df_second_stage[features_2]\n",
        "Y2 = df_second_stage[target_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a7f2195",
      "metadata": {
        "id": "4a7f2195"
      },
      "outputs": [],
      "source": [
        "model_2 = LogisticRegression()\n",
        "model_2.fit(X2, Y2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to a file\n",
        "joblib.dump(model_2, MODEL_2_PATH)"
      ],
      "metadata": {
        "id": "LwAoW1CnM1bO"
      },
      "id": "LwAoW1CnM1bO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "504f69bb",
      "metadata": {
        "id": "504f69bb"
      },
      "source": [
        "## 2 Retrieval\n",
        "\n",
        "Let's make the ranking of documents for a given query using the two-stage logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e28925db",
      "metadata": {
        "id": "e28925db"
      },
      "outputs": [],
      "source": [
        "# Ideally, these structures should be generated from a large\n",
        "# biomedical knowledge base. They are hardcoded here for simplicity.\n",
        "THESAURUS = {\n",
        "    'blood': [ 'bld', 'serum', 'ser', 'plasma', 'plas'],\n",
        "    'bilirubin': ['bilirubin'],\n",
        "    'plasma': ['plas'],\n",
        "    'white blood cells': ['wbc', 'leukocytes', 'lymphocytes', 'monocytes'],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Ideally, this mapping should be generated at runtime from the THESAURUS\n",
        "# This would be part of a large information retrieval module but it is\n",
        "# out of scope for this example.\n",
        "QUERY_TO_CONCEPTS = {\n",
        "    'glucose in blood': ['glucose', 'blood'],\n",
        "    'bilirubin in plasma': ['bilirubin', 'plasma'],\n",
        "    'white blood cells count': ['white blood cells','blood','cells','count'],\n",
        "    'carnitine in blood': ['carnitine','blood'],\n",
        "    'tyrosine in blood': ['tyrosine','blood']\n",
        "}\n",
        "\n",
        "# Getting our Corpus\n",
        "CORPUS_PATH = 'data/loinc_docs.csv'\n",
        "try:\n",
        "    df_corpus = pd.read_csv(CORPUS_PATH)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading corpus dataset: {e}\")\n",
        "    exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410e32f5",
      "metadata": {
        "id": "410e32f5"
      },
      "outputs": [],
      "source": [
        "import math, re\n",
        "\n",
        "def check_match(field, concept_terms):\n",
        "    return any(term.lower() in field.lower() for term in concept_terms)\n",
        "\n",
        "def check_appears_in_document(loinc_doc, concept_terms):\n",
        "    return check_match(loinc_doc['long_common_name'], concept_terms) or \\\n",
        "        check_match(loinc_doc['component'], concept_terms) or \\\n",
        "        check_match(loinc_doc['system'], concept_terms)\n",
        "\n",
        "def build_first_stage_dataset(concepts):\n",
        "\n",
        "    dataset_1_rows = []\n",
        "    for concept in concepts:\n",
        "        concept_terms = THESAURUS.get(concept)\n",
        "\n",
        "        for _, loinc_doc in df_corpus.iterrows():\n",
        "\n",
        "            # Check if any of the terms for the concept are in the document\n",
        "            if check_appears_in_document(loinc_doc, concept_terms):\n",
        "\n",
        "                # If there is a match, compute TF and other features\n",
        "                tf = sum(re.findall(r\"\\b\" + re.escape(term.lower()) + r\"\\b\", loinc_doc['long_common_name'].lower()) for term in concept_terms)\n",
        "                idf = math.log(len(df_corpus) / sum(1 for _, doc in df_corpus.iterrows() if check_appears_in_document(doc, concept_terms)))\n",
        "                is_in_component = int(check_match(loinc_doc['component'], concept_terms))\n",
        "                is_in_system = int(check_match(loinc_doc['system'], concept_terms))\n",
        "\n",
        "                dataset_1_rows.append({\n",
        "                    'loinc_num': loinc_doc['loinc_num'],\n",
        "                    'concept': concept,\n",
        "                    'TF': tf,\n",
        "                    'IDF': idf,\n",
        "                    'is_in_component': is_in_component,\n",
        "                    'is_in_system': is_in_system,\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(dataset_1_rows)\n",
        "\n",
        "def build_second_stage_dataset(df, query):\n",
        "\n",
        "    # Calculate Z score per document\n",
        "    Z_query = df.groupby(['loinc_num'])['log_odds'].sum().reset_index()\n",
        "    Z_query = Z_query.rename(columns={'log_odds': 'Z'})\n",
        "\n",
        "    # Calculate N (number of unique concepts) per document\n",
        "    N_query = df.groupby('loinc_num')['concept'].nunique().reset_index()\n",
        "    N_query = N_query.rename(columns={'concept': 'N'})\n",
        "\n",
        "    # Merge Z and N dataframes\n",
        "    df_second_stage = pd.merge(Z_query, N_query, on='loinc_num', how='left')\n",
        "\n",
        "    # Fill NaN values with 0\n",
        "    df_second_stage['Z'] = df_second_stage['Z'].fillna(0)\n",
        "    df_second_stage['N'] = df_second_stage['N'].fillna(0)\n",
        "\n",
        "    return df_second_stage\n",
        "\n",
        "def rank_documents(query):\n",
        "\n",
        "    # 1. Get the concepts for the query. Again this would be part of a larger\n",
        "    # information retrieval module.\n",
        "    concepts = QUERY_TO_CONCEPTS.get(query)\n",
        "\n",
        "    if not concepts:\n",
        "        print(f\"No concepts found for query: {query}\")\n",
        "        return None\n",
        "\n",
        "    # 2. Build dataset #1 for the query\n",
        "    df_first_stage_query = build_first_stage_dataset(concepts, query)\n",
        "\n",
        "    # 3. Get log-odds from first model\n",
        "    df_first_stage_query['log_odds'] = model_1.decision_function(df_first_stage_query[features_1])\n",
        "\n",
        "    # 4. Build second stage dataset\n",
        "    df_second_stage_query = build_second_stage_dataset(df_first_stage_query, query)\n",
        "\n",
        "    # 5. Predict relevance using second model\n",
        "    df_second_stage_query['final_score'] = model_2.decision_function(df_second_stage_query[features_2])\n",
        "\n",
        "    # 6. Rank documents based on final score\n",
        "    df_ranked = df_second_stage_query.sort_values(by='final_score', ascending=False)\n",
        "\n",
        "    return df_ranked[['loinc_num', 'long_common_name', 'final_score']]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_first_stage_dataset(concepts,query):\n",
        "  # Ensure elementary_clues_df is empty before filling\n",
        "  elementary_clues_df = pd.DataFrame(columns=['loinc_num', 'query_id', 'composite_clue', 'TF', 'IDF', 'is_in_component', 'is_in_system'])\n",
        "\n",
        "  # Calculate total number of documents\n",
        "  N = len(df_corpus)\n",
        "\n",
        "  # Calculate document frequency for original composites (for IDF of the composite itself)\n",
        "  doc_freq_composites = {}\n",
        "  for composite in concepts:\n",
        "      lower_composite = composite.lower()\n",
        "      # Get the composite and its synonyms, including the composite itself\n",
        "      terms_to_check_doc_freq = [lower_composite]\n",
        "      if composite in THESAURUS:\n",
        "          terms_to_check_doc_freq.extend([synonym.lower() for synonym in THESAURUS[composite]])\n",
        "\n",
        "      # Count the number of documents where ANY of the terms in this group appear (using whole word match)\n",
        "      count = df_corpus.apply(lambda row: any(re.search(r'\\b' + re.escape(term) + r'\\b', str(row['long_common_name']).lower()) or\n",
        "                                      re.search(r'\\b' + re.escape(term) + r'\\b', str(row['component']).lower()) or\n",
        "                                      re.search(r'\\b' + re.escape(term) + r'\\b', str(row['system']).lower()) for term in terms_to_check_doc_freq), axis=1).sum()\n",
        "      doc_freq_composites[composite] = count\n",
        "\n",
        "  # Pre-calculate terms to count for each original composite (composite + its synonyms)\n",
        "  terms_to_count_map = {}\n",
        "  for composite in concepts:\n",
        "      lower_composite = composite.lower()\n",
        "      terms = [lower_composite]\n",
        "      if composite in THESAURUS:\n",
        "          terms.extend([synonym.lower() for synonym in THESAURUS[composite]])\n",
        "      terms_to_count_map[composite] = terms\n",
        "\n",
        "\n",
        "  # Iterate over each row in the main DataFrame (df)\n",
        "  for index, row in df_corpus.iterrows():\n",
        "      loinc_num = row['loinc_num']\n",
        "      long_common_name = str(row['long_common_name']).lower()\n",
        "      component = str(row['component']).lower()\n",
        "      system = str(row['system']).lower()\n",
        "\n",
        "      # Iterate over each original composite in the concepts list\n",
        "      for composite in concepts:\n",
        "          lower_composite = composite.lower()\n",
        "          terms_to_count = terms_to_count_map[composite]\n",
        "\n",
        "          # Check if ANY term related to this composite (composite or synonyms) is present in the document (using whole word match)\n",
        "          any_term_found = any(re.search(r'\\b' + re.escape(term) + r'\\b', long_common_name) or\n",
        "                              re.search(r'\\b' + re.escape(term) + r'\\b', component) or\n",
        "                              re.search(r'\\b' + re.escape(term) + r'\\b', system) for term in terms_to_count)\n",
        "\n",
        "          # If at least one related term was found in the document\n",
        "          if any_term_found:\n",
        "              # Calculate the total counts for X1, X3, X4 by summing occurrences of all related terms in the document (using whole word match)\n",
        "              total_x1 = sum(len(re.findall(r'\\b' + re.escape(term) + r'\\b', long_common_name)) for term in terms_to_count)\n",
        "              x3 = int(sum(len(re.findall(r'\\b' + re.escape(term) + r'\\b', component)) for term in terms_to_count) > 0)\n",
        "              x4 = int(sum(len(re.findall(r'\\b' + re.escape(term) + r'\\b', system)) for term in terms_to_count) > 0)\n",
        "\n",
        "              # Calculate X2 (IDF) for the ORIGINAL composite term\n",
        "              x2 = np.log10(N / (doc_freq_composites.get(composite, 0)))\n",
        "\n",
        "              # Create a new row dictionary for the original composite in this document\n",
        "              new_row = {\n",
        "                  'loinc_num': loinc_num,\n",
        "                  'query_id': query,\n",
        "                  'composite_clue': composite,\n",
        "                  'TF': total_x1,\n",
        "                  'IDF': x2,\n",
        "                  'is_in_component': x3,\n",
        "                  'is_in_system': x4\n",
        "              }\n",
        "              # Add the new row to the elementary_clues_df DataFrame\n",
        "              elementary_clues_df.loc[len(elementary_clues_df)] = new_row\n",
        "\n",
        "  return elementary_clues_df"
      ],
      "metadata": {
        "id": "P8N24VO1cwVA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "P8N24VO1cwVA"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_second_stage_dataset(concepts, df, query):\n",
        "  # Create the second dataset DataFrame\n",
        "  second_dataset_df = pd.DataFrame(columns=['loinc_num', 'query_id', 'Z', 'N'])\n",
        "\n",
        "  # Iterate over each row in the original DataFrame (df)\n",
        "  for index, row in df_corpus.iterrows():\n",
        "      loinc_num = row['loinc_num']\n",
        "      long_common_name = str(row['long_common_name']).lower()\n",
        "      component = str(row['component']).lower()\n",
        "      system = str(row['system']).lower()\n",
        "\n",
        "      # Calculate N: number of unique composites or their thesaurus found in long_common_name\n",
        "      found_composites = set()\n",
        "      # Iterate over each original composite in the composites list\n",
        "      for composite in concepts:\n",
        "          lower_composite = composite.lower()\n",
        "          # Get the composite and its synonyms, including the composite itself\n",
        "          terms_to_check = [lower_composite]\n",
        "          if composite in THESAURUS:\n",
        "              terms_to_check.extend([synonym.lower() for synonym in THESAURUS[composite]])\n",
        "\n",
        "          # Check if ANY of the terms related to this composite (composite or synonyms) is present in the document (using whole word match)\n",
        "          if any(re.search(r'\\b' + re.escape(term) + r'\\b', long_common_name) for term in terms_to_check):\n",
        "              found_composites.add(composite)\n",
        "\n",
        "      z = df[(df['loinc_num'] == loinc_num)]\n",
        "      # print(loinc_num)\n",
        "      # dz = 0\n",
        "      # print(z)\n",
        "      if len(z) > 0:\n",
        "        dz = z['log_odds'].sum()\n",
        "\n",
        "      n_count = len(found_composites)\n",
        "\n",
        "      # Create a new row dictionary for the second dataset\n",
        "      new_row = {\n",
        "          'loinc_num': loinc_num,\n",
        "          'query_id': query,\n",
        "          'Z': dz,\n",
        "          'N': n_count\n",
        "      }\n",
        "\n",
        "      # Add the new row to the second_dataset_df DataFrame\n",
        "      second_dataset_df.loc[len(second_dataset_df)] = new_row\n",
        "\n",
        "  return second_dataset_df"
      ],
      "metadata": {
        "id": "a5xVS4x35N5L"
      },
      "execution_count": null,
      "outputs": [],
      "id": "a5xVS4x35N5L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab7oUMBdOjQY"
      },
      "outputs": [],
      "source": [
        "def rank_documents(query):\n",
        "\n",
        "    # 1. Get the concepts for the query. Again this would be part of a larger\n",
        "    # information retrieval module.\n",
        "    concepts = QUERY_TO_CONCEPTS.get(query)\n",
        "\n",
        "    if not concepts:\n",
        "        print(f\"No concepts found for query: {query}\")\n",
        "        return None\n",
        "\n",
        "    # 2. Build dataset #1 for the query\n",
        "    df_first_stage_query = build_first_stage_dataset(concepts, query)\n",
        "\n",
        "    # 3. Get log-odds from first model\n",
        "    df_first_stage_query['log_odds'] = model_1.decision_function(df_first_stage_query[features_1])\n",
        "    # display(df_first_stage_query)\n",
        "\n",
        "    # 4. Build second stage dataset\n",
        "    df_second_stage_query = build_second_stage_dataset(concepts,df_first_stage_query, query)\n",
        "    # display(df_second_stage_query)\n",
        "    # print(\"Llego\")\n",
        "\n",
        "    # 5. Predict relevance using second model\n",
        "    df_second_stage_query['final_score'] = model_2.decision_function(df_second_stage_query[features_2])\n",
        "\n",
        "    # Merge with df_corpus to get the 'long_common_name'\n",
        "    df_second_stage_query = pd.merge(df_second_stage_query, df_corpus[['loinc_num', 'long_common_name']], on='loinc_num', how='left')\n",
        "\n",
        "    # 6. Rank documents based on final score\n",
        "    df_ranked = df_second_stage_query.sort_values(by='final_score', ascending=False)\n",
        "\n",
        "    return df_ranked[['loinc_num', 'long_common_name', 'final_score']]"
      ],
      "id": "ab7oUMBdOjQY"
    },
    {
      "cell_type": "code",
      "source": [
        "search_query = \"tyrosine in blood\"\n",
        "num = 10\n",
        "ranked_list_1 = rank_documents(search_query)\n",
        "if ranked_list_1 is not None:\n",
        "    print(f\"--- Top {num} Results for '{search_query}' ---\")\n",
        "    print(ranked_list_1.head(num))"
      ],
      "metadata": {
        "id": "kgsdYaNyU2jE"
      },
      "id": "kgsdYaNyU2jE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rwd",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}